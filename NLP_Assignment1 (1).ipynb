{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "you are required to extract text from Html page and apply processing in the\n",
        "text then get the unique words from the text:\n",
        "• Data:\n",
        "1. USE any URL (e.g. Wikipedia),\n",
        "2. Extract HTML from URL\n",
        "3. Extract TEXT from HTML page e.g. (p or headings)\n",
        "All 3 steps in python\n",
        "• Processing on Data:\n",
        "1. Cleaning data from each symbol or character doesn’t contain to the data.\n",
        "2. Normalization: make all the data to lower case\n",
        "3. Tokenization: split the data to words\n",
        "4. Lemmatization or Stemming: return each word to origin.\n",
        "5. Stop words: remove stop words from the data.\n",
        "• Unique Word: the output of the assignment to get all the unique words on the data."
      ],
      "metadata": {
        "id": "SzIqBgMVw-mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFRWtWe8daSO",
        "outputId": "100f6b41-a584-4640-8e9d-699e908c63bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import wordsegment"
      ],
      "metadata": {
        "id": "9PRtKRTDpSQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Use URL to extract HTML\n",
        "def get_html_text(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "y-QYbCIFpVKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract text from HTML page\n",
        "def extract_text(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    text = \" \".join([p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])])\n",
        "    return text"
      ],
      "metadata": {
        "id": "CsxFyj_PpbJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data processing\n",
        "def process_data(text):\n",
        "    # Step 1: Clean data\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Step 2: Normalization (lowercasing)\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "\n",
        "    # Step 3: Tokenization\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "\n",
        "    # Step 4: Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "\n",
        "    # Step 5: Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "Y6TwdMWlpiNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Get unique words\n",
        "def get_unique_words(tokens):\n",
        "    unique_words = set(tokens)\n",
        "    return unique_words"
      ],
      "metadata": {
        "id": "HragwMqhpr47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "url = \"https://www.lipsum.com/\"\n",
        "html = get_html_text(url)\n",
        "text = extract_text(html)\n",
        "tokens = process_data(text)\n",
        "unique_words = get_unique_words(tokens)\n",
        "\n",
        "print(\"Number of unique words:\", len(unique_words))\n",
        "print(\"Unique words:\", unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxhJnbTqp1SV",
        "outputId": "fd3f647f-7ea9-408d-d61d-fbc69042ee96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 514\n",
            "Unique words: {'still', 'distribution', 'autem', 'avoid', 'old', 'power', 'model', 'bc', 'natus', 'dont', 'corporis', 'blame', 'dictum', 'wise', 'reprehenderit', 'find', 'occur', 'porro', 'anything', 'moment', 'aut', 'matter', 'voluptatum', 'aliquid', 'alias', 'trivial', 'web', 'right', 'annoying', 'college', 'variation', 'h', 'nostrum', 'hic', '45', 'isnt', 'treatise', 'dolore', 'chooses', 'vitae', 'possimus', 'non', 'aliquip', 'nulla', 'site', 'dislike', 'explain', 'doloremque', 'voluptates', 'evil', 'odio', 'nothing', 'eligendi', 'charm', 'give', 'remaining', 'leap', 'sit', 'quidem', 'architecto', 'nisi', 'want', 'believable', 'section', 'evolved', '1500s', 'passage', 'reader', 'professor', 'us', 'nam', 'painful', 'hampdensydney', 'delectus', 'aldus', 'blinded', 'eos', 'corrupti', 'quia', 'randomised', 'soluta', 'therefore', 'business', 'obscure', 'book', 'industry', 'ad', 'trouble', 'adipiscing', 'asperiores', 'praesentium', 'choice', 'frequently', 'truth', 'beatae', 'denounce', 'established', 'provident', 'totam', 'system', 'must', 'indignation', 'accusantium', 'accompanied', 'structure', 'consequuntur', 'shrinking', 'cupidatat', 'velit', 'mistaken', 'impedit', 'latin', 'man', 'proident', 'accusamus', 'et', 'hour', 'need', 'normal', 'voluptatibus', 'resultant', 'perspiciatis', 'omnis', 'facere', 'unde', 'earum', 'id', 'circumstance', 'free', 'neque', 'happiness', 'literature', 'making', 'quaerat', 'sunt', 'uncover', 'standard', 'chunk', 'nesciunt', 'commodo', 'cumque', 'bound', 'containing', '11033', 'scrambled', 'rem', 'looked', 'necessitatibus', 'voluptatem', 'magnam', 'eius', 'de', 'galley', 'cite', 'consequatur', 'adipisci', 'fact', 'exercise', 'veritatis', 'malorum', 'default', 'eum', 'century', 'numquam', 'est', 'secure', 'beguiled', 'u', 'prevents', 'simple', 'mollitia', 'consequence', 'fugiat', 'word', 'love', 'excepteur', 'handful', 'alteration', 'text', 'repellat', 'wa', 'discovered', 'long', 'ha', 'unknown', 'vel', 'survived', 'extreme', 'owing', 'desktop', 'ethic', 'pagemaker', 'account', 'generated', 'doe', 'generator', 'reiciendis', 'source', '2000', 'popularised', 'layout', 'content', 'use', 'original', 'quis', 'ex', 'rerum', 'elit', 'page', 'advantage', 'hidden', 'exact', 'sint', 'able', 'blanditiis', 'saepe', 'year', 'contrary', 'publishing', 'illo', 'noncharacteristic', 'sure', 'infancy', 'masterbuilder', 'always', 'repudiandae', 'fault', 'atque', 'version', 'human', 'except', 'sed', 'sometimes', 'excepturi', 'exercitation', 'modi', 'dummy', 'essentially', 'perfectly', 'readable', '1914', 'pleasure', 'going', 'selection', 'vero', 'repudiated', 'suffered', 'simply', 'consectetur', 'aliquam', 'dolorum', 'iste', 'electronic', 'ensue', 'molestias', 'obtain', 'classical', 'iure', 'dignissimos', 'also', 'translation', 'interested', 'officiis', 'ever', 'incidunt', 'tend', 'complete', 'amet', 'similique', 'lorem', 'great', 'nemo', 'occasionally', 'nihil', 'labore', 'sapiente', 'qui', 'facilis', 'seek', 'undoubtable', 'laboriosam', 'fuga', 'men', 'typesetting', 'generate', 'incididunt', 'greater', 'release', 'cupiditate', 'dolor', 'piece', 'humour', 'ratione', '11032', 'quod', 'desire', 'error', 'recently', 'pain', 'assumenda', 'endures', 'quibusdam', 'quasi', 'saying', 'untrammelled', 'worse', 'weakness', 'middle', 'praising', 'best', 'come', 'officia', 'editor', 'ea', 'first', 'deleniti', 'true', 'voluptate', 'placeat', 'molestiae', 'encounter', 'deserunt', 'minim', 'hold', 'iusto', 'avoided', 'reasonable', 'equal', 'ullamco', 'letraset', 'english', 'accident', 'five', 'explicabo', 'tempor', 'quam', 'physical', 'temporibus', 'suscipit', 'dolorem', 'cicero', 'duty', 'ipsam', 'richard', 'even', 'every', 'form', 'duis', 'root', 'inventore', 'software', 'reproduced', 'printing', 'voluptas', 'line', 'sentence', 'example', '1960s', 'point', 'commodi', 'nostrud', 'printer', 'explorer', 'laboris', 'combined', 'one', 'expound', 'maxime', 'repellendus', 'eu', 'optio', 'fugit', 'culpa', 'quae', 'ut', 'case', 'claim', 'irure', 'perferendis', 'righteous', 'renaissance', 'mcclintock', 'theory', 'enjoy', 'magna', 'cum', 'ab', 'ipsa', 'embarrassing', 'animi', 'easy', 'popular', 'recusandae', 'good', 'bonorum', 'necessary', 'teaching', 'fail', 'quisquam', 'type', 'predefined', 'cillum', 'ipsum', 'search', 'various', 'slightly', 'know', 'born', 'looking', 'sequi', 'esse', 'expedita', 'certain', 'opposed', 'repetition', 'using', 'doloribus', 'harum', 'annoyance', 'specimen', 'aliqua', 'get', 'dolores', 'written', 'odit', 'look', 'anyone', 'ullam', 'minus', 'obligation', 'quas', 'produce', 'maiores', 'package', 'pariatur', 'majority', 'itaque', 'enim', 'distracted', 'rationally', 'take', 'actual', 'belongs', 'else', 'rackham', 'eiusmod', 'laudantium', 'laborum', 'belief', 'pursues', 'mollit', 'demoralized', 'tempore', 'debitis', 'like', '200', 'accepted', 'purpose', 'quo', 'aute', 'moreorless', 'illum', 'repeat', 'injected', 'laborious', 'since', 'etc', 'undertakes', 'exercitationem', 'aspernatur', 'consequat', 'ducimus', 'dictionary', 'magni', 'avoids', 'took', 'idea', 'random', 'procure', 'welcomed', 'denouncing', 'internet', 'available', 'nobis', 'sheet', 'extremely', 'finibus', 'libero', 'minimum', 'pursue', 'distinguish', 'distinctio', 'aperiam', 'tenetur', 'principle', 'many', 'occaecati', 'including', 'used', 'veniam', 'unchanged', 'reject', 'letter', 'anim', 'hand', 'occaecat', 'eveniet', 'virginia', 'eaque', 'tempora', 'quos', 'foresee', 'make', 'toil'}\n"
          ]
        }
      ]
    }
  ]
}